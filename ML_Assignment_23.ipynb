{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are the key reasons for reducing the dimensionality of a dataset? What are the major disadvantages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Disadvantages of Dimensionality Reduction:\n",
    "    \n",
    "->It may lead to some amount of data loss.\n",
    "->PCA tends to find linear correlations between variables,which is sometimes undersirable.\n",
    "->PCA fails in cases where mean and covariance are not enough to define datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What is the dimensionality curse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The curse of dimensionality in machine learning is defined as follows, As the number of dimensions or features increases,\n",
    "the amount of data needed to generalize the machine learning model accurately increases exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how can you go about \n",
    "doing it? If not, what is the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "No,dimensionality reduction is not reversible in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Well it depends on dataset.If it is comprised of points that are perfectly aligned,PCA can reduce the dataset down to 1 \n",
    "dimension and preserve 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Assume you're running PCA on a 1,000-dimensional dataset with a 95 percent explained variance ratio. What is the number\n",
    "of dimensions that the resulting dataset would have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If i perform PCA on a 1,000-dimensional dataset,setting the explained variance ratio to 95% .In this case roughly 950 \n",
    "dimensions are required to preserved 95% of the variance.So the answer is,it depends on the dataset ,and it could be any \n",
    "number between 1 and 950."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The following are the secnarios where the following are used:\n",
    "\n",
    "->Vanilla PCA:the dataset fit in memory.\n",
    "->Incremental PCA:largest dataset that don't fit in memory,online tasks.\n",
    "->Randomized PCA:considerably reduce dimensionality and the dataset fit the memory.\n",
    "->kernal PCA:used for nonlinear PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7. How do you assess a dimensionality reduction algorithm's success on your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "By doing PCA,it is a good choice for dimensionality reduction and visualization for datasets with a large number of \n",
    "variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Is it logical to use two different dimensionality reduction algorithms in a chain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indeed,it often make any sense to chain two different dimensionality reduction algorithms.A common example is using PCA to\n",
    "quickly get rid of a large number of useless dimensions,then applying another much slower dimensionality reduction \n",
    "algorithm,such as LLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
