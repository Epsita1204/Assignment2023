{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the underlying concept of Support Vector Machines?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and \n",
    "non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or \n",
    "a hyperplane which separates the data into classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What is the concept of a support vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the \n",
    "hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change\n",
    "the position of the hyperplane. These are the points that help us build our SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3. When using SVMs, why is it necessary to scale the inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In Support Vector Machines (SVM), feature scaling or normalization are not strictly required, but are highly recommended, \n",
    "as it can significantly improve model performance and convergence speed. SVM tries to find the optimal hyperplane that \n",
    "separates the data points of different classes with the maximum margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4. When an SVM classifier classifies a case, can it output a confidence score? What about a percentage chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "An SVM classifier can output the distance between the test instance and the decision boundary, and you can use this as a \n",
    "confidence score. However, this score cannot be directly converted into an estimation of the class probability. If you set\n",
    "probability=True when creating an SVM in Scikit-Learn, then after training it will calibrate the probabilities using \n",
    "Logistic Regression on the SVM’s scores (trained by an additional five-fold cross-validation on the training data). This \n",
    "will add the predict_proba() and predict_log_proba() methods to the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Should you train a model on a training set with millions of instances and hundreds of features using the primal or dual \n",
    "form of the SVM problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This question applies only to linear SVMs since kernelized can only use the dual form. The computational complexity of the \n",
    "primal form of the SVM problem is proportional to the number of training instances m, while the computational complexity \n",
    "of the dual form is proportional to a number between m2 and m3. So if there are millions of instances, you should \n",
    "definitely use the primal form, because the dual form will be much too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Let's say you've used an RBF kernel to train an SVM classifier, but it appears to underfit the training collection. Is \n",
    "it better to raise or lower (gamma)? What about the letter C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If an SVM classifier trained with an RBF kernel underfits the training set, there might be too much regularization. To \n",
    "decrease it, you need to increase gamma or C (or both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7. To solve the soft margin linear SVM classifier problem with an off-the-shelf QP solver, how should the QP parameters \n",
    "(H, f, A, and b) be set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Let’s call the QP parameters for the hard-margin problem H′, f′, A′ and b′. The QP parameters for the soft-margin problem \n",
    "have m additional parameters (np = n + 1 + m) and m additional constraints (nc = 2m). They can be defined like so:\n",
    "\n",
    "1. H is equal to H′, plus m columns of 0s on the right and m rows of 0s at the bottom\n",
    "2. f is equal to f′ with m additional elements, all equal to the value of the hyperparameter C.\n",
    "3. b is equal to b′ with m additional elements, all equal to 0.\n",
    "4. A is equal to A′, with an extra m × m identity matrix Im appended to the right,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8. On a linearly separable dataset, train a LinearSVC. Then, using the same dataset, train an SVC and an SGDClassifier. \n",
    "See if you can get them to make a model that is similar to yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, (2, 3)] \n",
    "y=iris['target']\n",
    "setosa_or_versicolor = (y == 0) | (y == 1)\n",
    "X = X[setosa_or_versicolor]\n",
    "y = y[setosa_or_versicolor]\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "lin_clf = LinearSVC(loss='hinge', C=5, random_state=42) \n",
    "#SVM classifier \n",
    "svm_clf = SVC(kernel='linear', C=5)\n",
    "#SGDClassifier \n",
    "sgd_clf = SGDClassifier(loss='hinge', learning_rate='constant', eta0=0.001,max_iter=1000, tol=1e-3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9. On the MNIST dataset, train an SVM classifier. You'll need to use one-versus-the-rest to assign all\n",
    "10 digits because SVM classifiers are binary classifiers. To accelerate up the process, you might want\n",
    "to tune the hyperparameters using small validation sets. What level of precision can you achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10. On the California housing dataset, train an SVM regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "california=fetch_california_housing()\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(california.data,california.target,test_size=0.2,random_state=42)\n",
    "scaler=StandardScaler()\n",
    "X_train_scaled=scaler.fit_transform(X_train)\n",
    "X_test_scaled=scaler.fit_transform(X_test)\n",
    "svm_reg=SVR(kernel=\"rbf\")\n",
    "svm_reg.fit(X_train_scaled,Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
